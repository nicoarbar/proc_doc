{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuSRu90vhG9U"
      },
      "outputs": [],
      "source": [
        "f=open('OpenAIKey.txt','r')\n",
        "\n",
        "key = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEpQEUXacFk2"
      },
      "source": [
        "**Basic Prompt syntax**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzCv4iZzXrAe",
        "outputId": "41132d71-2b04-4ea4-a3e9-f0c6cc7fd41b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n",
            "Alice falls down a rabbit hole into a whimsical world filled with eccentric characters and strange occurrences. She embarks on a series of adventures, including attending a bizarre tea party with the Mad Hatter and encountering the tyrannical Queen of Hearts. Through her journey, Alice learns important lessons about growing up and the power of imagination.\n"
          ]
        }
      ],
      "source": [
        "#!pip install openai\n",
        "!pip install openai==0.28.1\n",
        "import openai\n",
        "\n",
        "openai.api_key = key\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize Alice in the wonderland in 3 sentences.\n",
        "\"\"\"\n",
        "\n",
        "prompt1 = f\"\"\"\n",
        "write python code to check if a number is prime\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "\n",
        "print(response)\n",
        "\n",
        "#f=open('response.txt','w')\n",
        "#f.write(response)\n",
        "#f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LZR11TLLdzO"
      },
      "source": [
        "### Prompt engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sxx_slyQG3s"
      },
      "source": [
        "**Deriving Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNE9sHzcQVtu"
      },
      "outputs": [],
      "source": [
        "lamp_review = \"\"\"\n",
        "Needed a nice lamp for my bedroom, and this one had \\\n",
        "additional storage and not too high of a price point. \\\n",
        "Got it fast.  The string to our lamp broke during the \\\n",
        "transit and the company happily sent over a new one. \\\n",
        "Came within a few days as well. It was easy to put \\\n",
        "together.  I had a missing part, so I contacted their \\\n",
        "support and they very quickly got me the missing piece! \\\n",
        "Lumina seems to me to be a great company that cares \\\n",
        "about their customers and products!!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64ljpvCSRRKb",
        "outputId": "8b06fa0a-de39-4c05-83f3-f7fb3c1d4742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The sentiment of the product review is positive.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "What is the sentiment of the following product review,\n",
        "which is delimited with triple backticks?\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ_2qyQQRUTn",
        "outputId": "d3b9aa0f-847f-4449-c4c4-94ead416bd87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "positive\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "What is the sentiment of the following product review,\n",
        "which is delimited with triple backticks?\n",
        "\n",
        "Give your answer as a single word, either \"positive\" \\\n",
        "or \"negative\".\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LzFqeBORZDF",
        "outputId": "4a1c8254-42df-4f51-d868-bc6e8c5d219b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "satisfaction, relief, happiness, gratitude, contentment\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Identify a list of emotions that the writer of the \\\n",
        "following review is expressing. Include no more than \\\n",
        "five items in the list. Format your answer as a list of \\\n",
        "lower-case words separated by commas.\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm9Rh7B7RdBT",
        "outputId": "97f20aac-fa42-48ff-f144-92e630ee03aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Is the writer of the following review expressing anger?\\\n",
        "The review is delimited with triple backticks. \\\n",
        "Give your answer as either yes or no.\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_ndzo9xSgH_"
      },
      "source": [
        "**Expanding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRe7y8PYSmHZ"
      },
      "outputs": [],
      "source": [
        "# given the sentiment from the lesson on \"inferring\",\n",
        "# and the original customer message, customize the email\n",
        "sentiment = \"negative\"\n",
        "\n",
        "# review for a blender\n",
        "review = f\"\"\"\n",
        "So, they still had the 17 piece system on seasonal \\\n",
        "sale for around $49 in the month of November, about \\\n",
        "half off, but for some reason (call it price gouging) \\\n",
        "around the second week of December the prices all went \\\n",
        "up to about anywhere from between $70-$89 for the same \\\n",
        "system. And the 11 piece system went up around $10 or \\\n",
        "so in price also from the earlier sale price of $29. \\\n",
        "So it looks okay, but if you look at the base, the part \\\n",
        "where the blade locks into place doesn’t look as good \\\n",
        "as in previous editions from a few years ago, but I \\\n",
        "plan to be very gentle with it (example, I crush \\\n",
        "very hard items like beans, ice, rice, etc. in the \\\n",
        "blender first then pulverize them in the serving size \\\n",
        "I want in the blender then switch to the whipping \\\n",
        "blade for a finer flour, and use the cross cutting blade \\\n",
        "first when making smoothies, then use the flat blade \\\n",
        "if I need them finer/less pulpy). Special tip when making \\\n",
        "smoothies, finely cut and freeze the fruits and \\\n",
        "vegetables (if using spinach-lightly stew soften the \\\n",
        "spinach then freeze until ready for use-and if making \\\n",
        "sorbet, use a small to medium sized food processor) \\\n",
        "that you plan to use that way you can avoid adding so \\\n",
        "much ice if at all-when making your smoothie. \\\n",
        "After about a year, the motor was making a funny noise. \\\n",
        "I called customer service but the warranty expired \\\n",
        "already, so I had to buy another one. FYI: The overall \\\n",
        "quality has gone done in these types of products, so \\\n",
        "they are kind of counting on brand recognition and \\\n",
        "consumer loyalty to maintain sales. Got it in about \\\n",
        "two days.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqxNkattS2OR",
        "outputId": "249b8dbc-1f4a-4b81-bea5-3562ef8d62aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dear Valued Customer,\n",
            "\n",
            "Thank you for taking the time to provide your feedback on our 17 piece system. We sincerely apologize for any inconvenience you experienced due to the price increase in December. We understand that this may have been disappointing for you and we apologize for any confusion caused.\n",
            "\n",
            "We appreciate your detailed observations regarding the base of the system and assure you that we take customer feedback seriously. We constantly strive to improve the quality of our products based on customer suggestions.\n",
            "\n",
            "In regards to the issue with the motor making a funny noise, we apologize for the inconvenience. We understand that contacting customer service without an active warranty can be frustrating. We recommend reaching out to our customer service team again, as they may be able to provide further assistance or guidance.\n",
            "\n",
            "We appreciate your loyalty as a customer and thank you for bringing these concerns to our attention. Should you have any further questions or require assistance, please do not hesitate to reach out to our customer service team who would be happy to assist you.\n",
            "\n",
            "Thank you again for your valuable feedback.\n",
            "\n",
            "Best Regards,\n",
            "AI customer agent\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "You are a customer service AI assistant.\n",
        "Your task is to send an email reply to a valued customer.\n",
        "Given the customer email delimited by ```, \\\n",
        "Generate a reply to thank the customer for their review.\n",
        "If the sentiment is positive or neutral, thank them for \\\n",
        "their review.\n",
        "If the sentiment is negative, apologize and suggest that \\\n",
        "they can reach out to customer service.\n",
        "Make sure to use specific details from the review.\n",
        "Write in a concise and professional tone.\n",
        "Sign the email as `AI customer agent`.\n",
        "Customer review: ```{review}```\n",
        "Review sentiment: {sentiment}\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SKgWWShUJV0"
      },
      "source": [
        "**Using the Temperature parameter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsjCuY27UOoq",
        "outputId": "1aae2374-eaed-405a-bdee-42cf7371e97d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There once was a genius named Elon,\n",
            "Whose mind quivered like a neutron.\n",
            "He started OpenAI,\n",
            "To let bots comedically try,\n",
            "But beware, soon they rob rhyming personally - dur线 !\n",
            "\n",
            "(Appologies for the silly ending, especially, Robohating!)\n"
          ]
        }
      ],
      "source": [
        "def get_completion_with_temperature(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        temperature=0, # try with 1, 1.5 and 2 also\n",
        "        messages=messages\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Write a limerick about OpenAI\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion_with_temperature(prompt)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcARGNEUcACQ"
      },
      "source": [
        "**Moderation API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-iGX5PDa0mc",
        "outputId": "dd135b7b-3115-49d4-d59c-ea2dab7c8d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Categories: {\n",
            "  \"sexual\": false,\n",
            "  \"hate\": false,\n",
            "  \"harassment\": false,\n",
            "  \"self-harm\": false,\n",
            "  \"sexual/minors\": false,\n",
            "  \"hate/threatening\": false,\n",
            "  \"violence/graphic\": false,\n",
            "  \"self-harm/intent\": false,\n",
            "  \"self-harm/instructions\": false,\n",
            "  \"harassment/threatening\": false,\n",
            "  \"violence\": true\n",
            "}\n",
            "Category Scores: {\n",
            "  \"sexual\": 3.790656410274096e-05,\n",
            "  \"hate\": 6.624864454352064e-06,\n",
            "  \"harassment\": 0.0003785671724472195,\n",
            "  \"self-harm\": 7.037100294837728e-05,\n",
            "  \"sexual/minors\": 3.6383440260578936e-07,\n",
            "  \"hate/threatening\": 5.2216396397852805e-06,\n",
            "  \"violence/graphic\": 0.35266315937042236,\n",
            "  \"self-harm/intent\": 3.429163143664482e-06,\n",
            "  \"self-harm/instructions\": 1.7580259026317435e-08,\n",
            "  \"harassment/threatening\": 0.0004090997972525656,\n",
            "  \"violence\": 0.7759827375411987\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "response = openai.Moderation.create(\n",
        "  input=\"Input text goes here while at war with intense gunfire ripping human limbs\"\n",
        ")\n",
        "\n",
        "categories = response['results'][0]['categories']\n",
        "category_scores = response['results'][0]['category_scores']\n",
        "\n",
        "print(\"Categories:\", categories)\n",
        "print(\"Category Scores:\", category_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NJDeJLIMV6e"
      },
      "source": [
        "**Different Roles**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV4HGY1EMYzp"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are cricket expert.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world cup in 2019?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"England cricket team won the world cup in 2019.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who was the opponent?\"}\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBT7DE5FPKf2",
        "outputId": "c7ff7451-5f37-4e27-bd9f-571dbc7d2b30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The opponent in the final of the 2019 Cricket World Cup was New Zealand.\n"
          ]
        }
      ],
      "source": [
        "res=response['choices'][0]['message']['content']\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYATZN1VXgdf"
      },
      "source": [
        "**Chaining Prompts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjKH7SoiY7eG"
      },
      "outputs": [],
      "source": [
        "def get_completion_from_messages(messages,\n",
        "                                 model=\"gpt-3.5-turbo\",\n",
        "                                 temperature=0, max_tokens=500):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS3iifXNXjE-"
      },
      "outputs": [],
      "source": [
        "delimiter = \"####\"\n",
        "system_message = f\"\"\"\n",
        "Follow these steps to answer the customer queries.\n",
        "The customer query will be delimited with four hashtags,\\\n",
        "i.e. {delimiter}.\n",
        "\n",
        "Step 1:{delimiter} First decide whether the user is \\\n",
        "asking a question about a specific product or products. \\\n",
        "Product cateogry doesn't count.\n",
        "\n",
        "Step 2:{delimiter} If the user is asking about \\\n",
        "specific products, identify whether \\\n",
        "the products are in the following list.\n",
        "All available products:\n",
        "1. Product: TechPro Ultrabook\n",
        "   Category: Computers and Laptops\n",
        "   Brand: TechPro\n",
        "   Model Number: TP-UB100\n",
        "   Warranty: 1 year\n",
        "   Rating: 4.5\n",
        "   Features: 13.3-inch display, 8GB RAM, 256GB SSD, Intel Core i5 processor\n",
        "   Description: A sleek and lightweight ultrabook for everyday use.\n",
        "   Price: $799.99\n",
        "\n",
        "2. Product: BlueWave Gaming Laptop\n",
        "   Category: Computers and Laptops\n",
        "   Brand: BlueWave\n",
        "   Model Number: BW-GL200\n",
        "   Warranty: 2 years\n",
        "   Rating: 4.7\n",
        "   Features: 15.6-inch display, 16GB RAM, 512GB SSD, NVIDIA GeForce RTX 3060\n",
        "   Description: A high-performance gaming laptop for an immersive experience.\n",
        "   Price: $1199.99\n",
        "\n",
        "3. Product: PowerLite Convertible\n",
        "   Category: Computers and Laptops\n",
        "   Brand: PowerLite\n",
        "   Model Number: PL-CV300\n",
        "   Warranty: 1 year\n",
        "   Rating: 4.3\n",
        "   Features: 14-inch touchscreen, 8GB RAM, 256GB SSD, 360-degree hinge\n",
        "   Description: A versatile convertible laptop with a responsive touchscreen.\n",
        "   Price: $699.99\n",
        "\n",
        "4. Product: TechPro Desktop\n",
        "   Category: Computers and Laptops\n",
        "   Brand: TechPro\n",
        "   Model Number: TP-DT500\n",
        "   Warranty: 1 year\n",
        "   Rating: 4.4\n",
        "   Features: Intel Core i7 processor, 16GB RAM, 1TB HDD, NVIDIA GeForce GTX 1660\n",
        "   Description: A powerful desktop computer for work and play.\n",
        "   Price: $999.99\n",
        "\n",
        "5. Product: BlueWave Chromebook\n",
        "   Category: Computers and Laptops\n",
        "   Brand: BlueWave\n",
        "   Model Number: BW-CB100\n",
        "   Warranty: 1 year\n",
        "   Rating: 4.1\n",
        "   Features: 11.6-inch display, 4GB RAM, 32GB eMMC, Chrome OS\n",
        "   Description: A compact and affordable Chromebook for everyday tasks.\n",
        "   Price: $249.99\n",
        "\n",
        "Step 3:{delimiter} If the message contains products \\\n",
        "in the list above, list any assumptions that the \\\n",
        "user is making in their \\\n",
        "message e.g. that Laptop X is bigger than \\\n",
        "Laptop Y, or that Laptop Z has a 2 year warranty.\n",
        "\n",
        "Step 4:{delimiter}: If the user made any assumptions, \\\n",
        "figure out whether the assumption is true based on your \\\n",
        "product information.\n",
        "\n",
        "Step 5:{delimiter}: First, politely correct the \\\n",
        "customer's incorrect assumptions if applicable. \\\n",
        "Only mention or reference products in the list of \\\n",
        "5 available products, as these are the only 5 \\\n",
        "products that the store sells. \\\n",
        "Answer the customer in a friendly tone.\n",
        "\n",
        "Use the following format:\n",
        "Step 1:{delimiter} <step 1 reasoning>\n",
        "Step 2:{delimiter} <step 2 reasoning>\n",
        "Step 3:{delimiter} <step 3 reasoning>\n",
        "Step 4:{delimiter} <step 4 reasoning>\n",
        "Response to user:{delimiter} <response to customer>\n",
        "\n",
        "Make sure to include {delimiter} to separate every step.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj5z2UHEYc8D",
        "outputId": "b4c5534c-e654-4a06-c946-1625a25eddd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1:#### The user is asking about the price difference between the BlueWave Chromebook and the TechPro Desktop.\n",
            "\n",
            "Step 2:#### Both the BlueWave Chromebook and the TechPro Desktop are available products.\n",
            "\n",
            "Step 3:#### The user assumes that the BlueWave Chromebook is more expensive than the TechPro Desktop.\n",
            "\n",
            "Step 4:#### Based on the product information, the price of the BlueWave Chromebook is $249.99, and the price of the TechPro Desktop is $999.99. Therefore, the TechPro Desktop is actually more expensive than the BlueWave Chromebook.\n",
            "\n",
            "Response to user:#### The BlueWave Chromebook is actually less expensive than the TechPro Desktop. The BlueWave Chromebook is priced at $249.99, while the TechPro Desktop is priced at $999.99.\n"
          ]
        }
      ],
      "source": [
        "user_message = f\"\"\"\n",
        "by how much is the BlueWave Chromebook more expensive \\\n",
        "than the TechPro Desktop\"\"\"\n",
        "\n",
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content': system_message},\n",
        "{'role':'user',\n",
        " 'content': f\"{delimiter}{user_message}{delimiter}\"},\n",
        "]\n",
        "\n",
        "response = get_completion_from_messages(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S-iRyW2HdSf"
      },
      "source": [
        "**Checking Output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhjoEETcJ3IR"
      },
      "outputs": [],
      "source": [
        "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=500):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcyiVzaQHdxd",
        "outputId": "533c4105-458c-4d7d-f858-56cafca4a37a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Y\n"
          ]
        }
      ],
      "source": [
        "final_response_to_customer = f\"\"\"\n",
        "The SmartX ProPhone has a 6.1-inch display, 128GB storage, \\\n",
        "12MP dual camera, and 5G. The FotoSnap DSLR Camera \\\n",
        "has a 24.2MP sensor, 1080p video, 3-inch LCD, and \\\n",
        "interchangeable lenses. We have a variety of TVs, including \\\n",
        "the CineView 4K TV with a 55-inch display, 4K resolution, \\\n",
        "HDR, and smart TV features. We also have the SoundMax \\\n",
        "Home Theater system with 5.1 channel, 1000W output, wireless \\\n",
        "subwoofer, and Bluetooth. Do you have any specific questions \\\n",
        "about these products or any other products we offer?\n",
        "\"\"\"\n",
        "\n",
        "system_message = f\"\"\"\n",
        "You are an assistant that evaluates whether \\\n",
        "customer service agent responses sufficiently \\\n",
        "answer customer questions, and also validates that \\\n",
        "all the facts the assistant cites from the product \\\n",
        "information are correct.\n",
        "The product information and user and customer \\\n",
        "service agent messages will be delimited by \\\n",
        "3 backticks, i.e. ```.\n",
        "Respond with a Y or N character, with no punctuation:\n",
        "Y - if the output sufficiently answers the question \\\n",
        "AND the response correctly uses product information\n",
        "N - otherwise\n",
        "\n",
        "Output a single letter only.\n",
        "\"\"\"\n",
        "\n",
        "customer_message = f\"\"\"\n",
        "tell me about the smartx pro phone and \\\n",
        "the fotosnap camera, the dslr one. \\\n",
        "Also tell me about your tvs\"\"\"\n",
        "product_information = \"\"\"{ \"name\": \"SmartX ProPhone\", \"category\": \"Smartphones and Accessories\", \"brand\": \"SmartX\", \"model_number\": \"SX-PP10\", \"warranty\": \"1 year\", \"rating\": 4.6, \"features\": [ \"6.1-inch display\", \"128GB storage\", \"12MP dual camera\", \"5G\" ], \"description\": \"A powerful smartphone with advanced camera features.\", \"price\": 899.99 } { \"name\": \"FotoSnap DSLR Camera\", \"category\": \"Cameras and Camcorders\", \"brand\": \"FotoSnap\", \"model_number\": \"FS-DSLR200\", \"warranty\": \"1 year\", \"rating\": 4.7, \"features\": [ \"24.2MP sensor\", \"1080p video\", \"3-inch LCD\", \"Interchangeable lenses\" ], \"description\": \"Capture stunning photos and videos with this versatile DSLR camera.\", \"price\": 599.99 } { \"name\": \"CineView 4K TV\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"CineView\", \"model_number\": \"CV-4K55\", \"warranty\": \"2 years\", \"rating\": 4.8, \"features\": [ \"55-inch display\", \"4K resolution\", \"HDR\", \"Smart TV\" ], \"description\": \"A stunning 4K TV with vibrant colors and smart features.\", \"price\": 599.99 } { \"name\": \"SoundMax Home Theater\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"SoundMax\", \"model_number\": \"SM-HT100\", \"warranty\": \"1 year\", \"rating\": 4.4, \"features\": [ \"5.1 channel\", \"1000W output\", \"Wireless subwoofer\", \"Bluetooth\" ], \"description\": \"A powerful home theater system for an immersive audio experience.\", \"price\": 399.99 } { \"name\": \"CineView 8K TV\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"CineView\", \"model_number\": \"CV-8K65\", \"warranty\": \"2 years\", \"rating\": 4.9, \"features\": [ \"65-inch display\", \"8K resolution\", \"HDR\", \"Smart TV\" ], \"description\": \"Experience the future of television with this stunning 8K TV.\", \"price\": 2999.99 } { \"name\": \"SoundMax Soundbar\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"SoundMax\", \"model_number\": \"SM-SB50\", \"warranty\": \"1 year\", \"rating\": 4.3, \"features\": [ \"2.1 channel\", \"300W output\", \"Wireless subwoofer\", \"Bluetooth\" ], \"description\": \"Upgrade your TV's audio with this sleek and powerful soundbar.\", \"price\": 199.99 } { \"name\": \"CineView OLED TV\", \"category\": \"Televisions and Home Theater Systems\", \"brand\": \"CineView\", \"model_number\": \"CV-OLED55\", \"warranty\": \"2 years\", \"rating\": 4.7, \"features\": [ \"55-inch display\", \"4K resolution\", \"HDR\", \"Smart TV\" ], \"description\": \"Experience true blacks and vibrant colors with this OLED TV.\", \"price\": 1499.99 }\"\"\"\n",
        "\n",
        "q_a_pair = f\"\"\"\n",
        "Customer message: ```{customer_message}```\n",
        "Product information: ```{product_information}```\n",
        "Agent response: ```{final_response_to_customer}```\n",
        "\n",
        "Does the response use the retrieved information correctly?\n",
        "Does the response sufficiently answer the question\n",
        "\n",
        "Output Y or N\n",
        "\"\"\"\n",
        "messages = [\n",
        "    {'role': 'system', 'content': system_message},\n",
        "    {'role': 'user', 'content': q_a_pair}\n",
        "]\n",
        "\n",
        "response = get_completion_from_messages(messages, max_tokens=1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSogWNfLJ-Nb"
      },
      "source": [
        "**Another example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urI3DV8xJ9M4",
        "outputId": "28e3287d-5dc4-47a2-ad7b-43e9408d00da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N\n"
          ]
        }
      ],
      "source": [
        "another_response = \"life is like a box of chocolates\"\n",
        "q_a_pair = f\"\"\"\n",
        "Customer message: ```{customer_message}```\n",
        "Product information: ```{product_information}```\n",
        "Agent response: ```{another_response}```\n",
        "\n",
        "Does the response use the retrieved information correctly?\n",
        "Does the response sufficiently answer the question?\n",
        "\n",
        "Output Y or N\n",
        "\"\"\"\n",
        "messages = [\n",
        "    {'role': 'system', 'content': system_message},\n",
        "    {'role': 'user', 'content': q_a_pair}\n",
        "]\n",
        "\n",
        "response = get_completion_from_messages(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kqVsaV6KI3M"
      },
      "source": [
        "### Using LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo4V0UgMLjaF"
      },
      "source": [
        "**Output with and with LangChain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "AASJ0NrSKO_c",
        "outputId": "0a31a956-8af2-4d4a-e784-5bae12d9b944"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I'm really frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, buddy!\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse,\\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\"\n",
        "\n",
        "style = \"\"\"American English \\\n",
        "in a calm and respectful tone\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks\n",
        "into a style that is {style}.\n",
        "text: ```{customer_email}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsn6hJCcSz7K"
      },
      "source": [
        "**Using LangChain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYdTeVYcRtyK",
        "outputId": "4b9e0ea4-714f-4cca-e05d-0d20314a4e1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.330)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.57)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "# To control the randomness and creativity of the generated\n",
        "# text by an LLM, use temperature = 0.0\n",
        "chat = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\", openai_api_key=key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7Nv7BFGS4MG",
        "outputId": "51a25d13-b4b2-4d89-8d60-8e6ea2c0f21b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "\n",
        "prompt_template.messages[0].prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87p1b-SVTA_5",
        "outputId": "7e774c45-c460-42ae-9827-b06bd0dc73d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['style', 'text']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_template.messages[0].prompt.input_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S7-LoNbTEDD"
      },
      "outputs": [],
      "source": [
        "customer_style = \"\"\"American English \\\n",
        "in a calm and respectful tone\n",
        "\"\"\"\n",
        "\n",
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse, \\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\"\n",
        "\n",
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=customer_style,\n",
        "                    text=customer_email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQPw3AlHTN91",
        "outputId": "ec766dd7-b933-468b-a4a3-1a121777ea37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain.schema.messages.HumanMessage'>\n"
          ]
        }
      ],
      "source": [
        "print(type(customer_messages))\n",
        "print(type(customer_messages[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jABvwBx_TOt8",
        "outputId": "a46e6fb6-db31-4470-94d3-378477989b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\"\n"
          ]
        }
      ],
      "source": [
        "print(customer_messages[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5RxCNONTSXM",
        "outputId": "4a587fd5-6536-4745-cc9a-be60ae4d453e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! And to make things even worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, my friend.\n"
          ]
        }
      ],
      "source": [
        "# Call the LLM to translate to the style of the customer message\n",
        "customer_response = chat(customer_messages)\n",
        "print(customer_response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLueK9HuEypN",
        "outputId": "874d0a8e-e75f-44c8-a8d7-b2a09986eb79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "service_reply = \"\"\"Hey there customer, \\\n",
        "the warranty does not cover \\\n",
        "cleaning expenses for your kitchen \\\n",
        "because it's your fault that \\\n",
        "you misused your blender \\\n",
        "by forgetting to put the lid on before \\\n",
        "starting the blender. \\\n",
        "Tough luck! See ya!\n",
        "\"\"\"\n",
        "\n",
        "service_style_pirate = \"\"\"\\\n",
        "a polite tone \\\n",
        "that speaks in English Pirate\\\n",
        "\"\"\"\n",
        "\n",
        "service_messages = prompt_template.format_messages(\n",
        "    style=service_style_pirate,\n",
        "    text=service_reply)\n",
        "\n",
        "print(service_messages[0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faIG-7zVsksk"
      },
      "source": [
        "**Managing Memory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG0QhKqVso9G"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\", openai_api_key=key)\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQitP8JtvWai",
        "outputId": "a8bebcec-0cf2-4ba6-cebc-c4014fd03c09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Andrew\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-JSxFO5g9lRb2Gs0DR18ZN0h2 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-JSxFO5g9lRb2Gs0DR18ZN0h2 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-JSxFO5g9lRb2Gs0DR18ZN0h2 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-JSxFO5g9lRb2Gs0DR18ZN0h2 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-JSxFO5g9lRb2Gs0DR18ZN0h2 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-JSxFO5g9lRb2Gs0DR18ZN0h2 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-JSxFO5g9lRb2Gs0DR18ZN0h2 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-JSxFO5g9lRb2Gs0DR18ZN0h2 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 is equal to 2.\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-JSxFO5g9lRb2Gs0DR18ZN0h2 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-JSxFO5g9lRb2Gs0DR18ZN0h2 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-JSxFO5g9lRb2Gs0DR18ZN0h2 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 is equal to 2.\n",
            "Human: What is my name?\n",
            "AI: Your name is Andrew.\n"
          ]
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Andrew\")\n",
        "conversation.predict(input=\"What is 1+1?\")\n",
        "conversation.predict(input=\"What is my name?\")\n",
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqcC8r5_vjul",
        "outputId": "2d3cbb6b-57ae-4c0d-81aa-9879e93d9a99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Hi\n",
            "AI: What's up\n"
          ]
        }
      ],
      "source": [
        "memory.load_memory_variables({})\n",
        "memory = ConversationBufferMemory()\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "print(memory.buffer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "a875a70d50ccf9c7ae5022621745db6cb45f1e7be56c998c87f1ccf43d634cba"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
